{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagged Time-Series Clustering Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwhall/anaconda3/envs/ananke/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/mwhall/anaconda3/envs/ananke/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from ananke._ddtw import DDTW\n",
    "from ananke._database_rework import TimeSeriesData\n",
    "from ananke._ts_simulation import gen_table\n",
    "from DBloomSCAN import BloomDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate data according to the above parameters, and insert it into an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(timepoints, nclust, nts_per_clust, nsr, shift_amount):\n",
    "    if os.path.isfile(\"simulation.h5\"):\n",
    "        os.remove(\"simulation.h5\")\n",
    "    tsdata = TimeSeriesData(\"simulation.h5\")\n",
    "    dataset_names = tsdata.initialize_by_shape(timepoints=timepoints)\n",
    "    nnoise = int(nsr*nclust*nts_per_clust)\n",
    "    sim = gen_table(fl_sig=0, w_sig=6,\n",
    "                    fl_bg=-6, w_bg=6,\n",
    "                    bg_disp_mu=0, bg_disp_sigma=1,\n",
    "                    sig_disp_mu2=0, sig_disp_sigma2=signal_variance,\n",
    "                    n_clust=nclust, n_sig=nts_per_clust, n_tax_sig=1, n_bg=nnoise,\n",
    "                    len_arima=2*nsamples, len_ts=nsamples, len_signal=nsamples-shift_amount)\n",
    "    X = sim['table']\n",
    "    Y = sim['signals']\n",
    "\n",
    "    tsdata.register_timeseries([\"ts%d\" % (x,) for x in range(nclust*nts_per_clust+nnoise)])\n",
    "    i = 0\n",
    "    for ts in X:\n",
    "        tsdata.set_timeseries_data(data=X[i,:], index=i, action='replace')\n",
    "        i += 1\n",
    "    del tsdata\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the data using a given distance function. Can do this in memory, or pull the data from disk. Only necessary for really large sets or my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(distance_function, in_memory=True):\n",
    "    print(\"Loading HDF5 file\")\n",
    "    tsd = TimeSeriesData(\"simulation.h5\")\n",
    "    n_objects = tsd._h5t[\"data/timeseries/matrix\"].shape[0]\n",
    "    time_points = [int(x) for x in tsd._h5t[\"data/timeseries/time\"][:]]\n",
    "    time_delta = np.array(time_points[1:]) - np.array(time_points[0:-1])\n",
    "    if in_memory:\n",
    "        data_matrix = np.empty(tsd._h5t[\"data/timeseries/matrix\"].shape)\n",
    "        print(\"Loading data matrix into memory\")\n",
    "        tsd._h5t[\"data/timeseries/matrix\"].read_direct(data_matrix)\n",
    "        def retrieve_data(index):\n",
    "            #If the data is too large for RAM, this can be swapped around to read from disk\n",
    "            #data = tsd._h5t[\"data/timeseries/matrix\"][index, :]\n",
    "            #return return data/sum(data)\n",
    "            return data_matrix[index,:]/sum(data_matrix[index,:])\n",
    "    else:\n",
    "        def retrieve_data(index):\n",
    "            #If the data is too large for RAM, this can be swapped around to read from disk\n",
    "            data = tsd._h5t[\"data/timeseries/matrix\"][index, :]\n",
    "            return data/sum(data)\n",
    "\n",
    "    print(\"Initializing BloomDistance structure\")\n",
    "    bd = BloomDistance(n_objects, distance_function, retrieve_data, \n",
    "                       dist_min = 0.001, dist_max=0.15, dist_step=0.005)\n",
    "    print(\"Pre-computing distances\")\n",
    "    #This should be set to 1 unless you're using DDTW, but I think that crashes anyways.\n",
    "    #Worth a shot somewhere with more RAM.\n",
    "    bd.compute_distances(n_threads=1)\n",
    "    return bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tunable parameters\n",
    "\n",
    "nsamples = 180\n",
    "nclust = 50\n",
    "nts_per_clust = 10\n",
    "#noise to signal ratio is 1:1\n",
    "nsr = 2\n",
    "shift_amount = 10\n",
    "signal_variance = 1.75\n",
    "#Options are \"sts\",\"dtw\", or \"ddtw\"\n",
    "distance_measure = \"sts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.286982\n",
      "Pre-computing distances\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.848667\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "Bloom filter '0.131' hit capacity, closing\n",
      "Bloom filter '0.126' hit capacity, closing\n",
      "Bloom filter '0.121' hit capacity, closing\n",
      "Bloom filter '0.116' hit capacity, closing\n",
      "Bloom filter '0.111' hit capacity, closing\n",
      "Bloom filter '0.106' hit capacity, closing\n",
      "Bloom filter '0.101' hit capacity, closing\n",
      "Bloom filter '0.096' hit capacity, closing\n",
      "Bloom filter '0.091' hit capacity, closing\n",
      "Bloom filter '0.086' hit capacity, closing\n",
      "Bloom filter '0.081' hit capacity, closing\n",
      "Bloom filter '0.076' hit capacity, closing\n",
      "Bloom filter '0.071' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.392382\n",
      "Pre-computing distances\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.787949\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "Bloom filter '0.131' hit capacity, closing\n",
      "Bloom filter '0.126' hit capacity, closing\n",
      "Bloom filter '0.121' hit capacity, closing\n",
      "Bloom filter '0.116' hit capacity, closing\n",
      "Bloom filter '0.111' hit capacity, closing\n",
      "Bloom filter '0.106' hit capacity, closing\n",
      "Bloom filter '0.101' hit capacity, closing\n",
      "Bloom filter '0.096' hit capacity, closing\n",
      "Bloom filter '0.091' hit capacity, closing\n",
      "Bloom filter '0.086' hit capacity, closing\n",
      "Bloom filter '0.081' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.644412\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "Bloom filter '0.131' hit capacity, closing\n",
      "Bloom filter '0.126' hit capacity, closing\n",
      "Bloom filter '0.121' hit capacity, closing\n",
      "Bloom filter '0.116' hit capacity, closing\n",
      "Bloom filter '0.111' hit capacity, closing\n",
      "Bloom filter '0.106' hit capacity, closing\n",
      "Bloom filter '0.101' hit capacity, closing\n",
      "Bloom filter '0.096' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.518237\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "Bloom filter '0.131' hit capacity, closing\n",
      "Bloom filter '0.126' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.452635\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.588222\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "Bloom filter '0.131' hit capacity, closing\n",
      "Bloom filter '0.126' hit capacity, closing\n",
      "Bloom filter '0.121' hit capacity, closing\n",
      "Bloom filter '0.116' hit capacity, closing\n",
      "Bloom filter '0.111' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.548871\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "Bloom filter '0.131' hit capacity, closing\n",
      "Bloom filter '0.126' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n",
      "Creating required data sets in new HDF5 file at simulation.h5\n",
      "Loading HDF5 file\n",
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n",
      "After 3000 samples of the distances, the max distance was 0.531409\n",
      "Pre-computing distances\n",
      "Bloom filter '0.146' hit capacity, closing\n",
      "Bloom filter '0.141' hit capacity, closing\n",
      "Bloom filter '0.136' hit capacity, closing\n",
      "Bloom filter '0.131' hit capacity, closing\n",
      "Bloom filter '0.126' hit capacity, closing\n",
      "Bloom filter '0.121' hit capacity, closing\n",
      "100.00%\n",
      "Done!\n",
      "Finding the nearest neighbours of each signal\n",
      "Computing scores\n"
     ]
    }
   ],
   "source": [
    "param_scores = {}\n",
    "\n",
    "#for nclust in [1,5,25,50,100,250]:\n",
    "for i in range(10):\n",
    "    param = i\n",
    "    timepoints = np.array(np.cumsum([random.randint(1,15) for i in range(nsamples)]))\n",
    "    \n",
    "    def compute_ddtw_distance(data1, data2):\n",
    "        distance, path = DDTW(data1, data2)\n",
    "        distance = distance[-1, -1]\n",
    "        return distance\n",
    "\n",
    "    def compute_dtw_distance(data1, data2):\n",
    "        distance, path = fastdtw(data1, data2)\n",
    "        return distance\n",
    "\n",
    "    #If we precompute this here, it saves doing it a few million more times\n",
    "    time_delta = timepoints[1:] - timepoints[0:-1]\n",
    "    def compute_sts_distance(data1, data2):\n",
    "        data1_delta = np.array(data1[1:]) - np.array(data1[0:-1])\n",
    "        data2_delta = np.array(data2[1:]) - np.array(data2[0:-1])\n",
    "        data1_slope = data1_delta / time_delta\n",
    "        data2_slope = data2_delta / time_delta\n",
    "        distance = data1_slope - data2_slope\n",
    "        distance = np.square(distance)\n",
    "        distance = np.sqrt(sum(distance))\n",
    "        return distance\n",
    "\n",
    "    if distance_measure == \"sts\":\n",
    "        distance_function = compute_sts_distance\n",
    "    elif distance_measure == \"dtw\":\n",
    "        distance_function = compute_dtw_distance\n",
    "    elif distance_measure == \"ddtw\":\n",
    "        distance_function = compute_ddtw_distance\n",
    "\n",
    "    true_signal = generate_data(timepoints, nclust, nts_per_clust, nsr, shift_amount)\n",
    "    dists = compute_distances(distance_function)\n",
    "\n",
    "    #Load once so we don't load it a billion times\n",
    "    tsd = TimeSeriesData(\"simulation.h5\")\n",
    "    data_matrix = np.empty(tsd._h5t[\"data/timeseries/matrix\"].shape)\n",
    "    tsd._h5t[\"data/timeseries/matrix\"].read_direct(data_matrix)\n",
    "\n",
    "    def find_nearest_timeseries(query_data, data_matrix, distance_function, n_threads = 1, n_chunks = 100):\n",
    "        query_data = query_data/sum(query_data)\n",
    "        min_distance = 999\n",
    "        min_index = None\n",
    "        p_distance_function = partial(distance_function, data2=query_data/sum(query_data))\n",
    "        # Break the data_matrix into chunks in case the source is on disk\n",
    "        def chunks(N, nb):\n",
    "            step = N / nb\n",
    "            return [(round(step*i), round(step*(i+1))) for i in range(nb)]\n",
    "        for i, j in chunks(data_matrix.shape[0], n_chunks):\n",
    "            data = data_matrix[i:j, :]\n",
    "            for k in range(i, j):\n",
    "                row = data[k-i, :]\n",
    "            distance = distance_function(query_data, row/sum(row))\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                min_index = k\n",
    "        return min_index\n",
    "\n",
    "    def score(true_signal, dists, data_matrix, nclust, nts_per_clust, nsr, n_threads = 1):\n",
    "        nnoise = int(nsr * nclust * nts_per_clust)\n",
    "        scores = {}\n",
    "        nearest_neighbour = {}\n",
    "        print(\"Finding the nearest neighbours of each signal\")\n",
    "        for i in range(true_signal.shape[0]):\n",
    "            nearest_index = find_nearest_timeseries(true_signal[i,:]/sum(true_signal[i,:]), \n",
    "                                                    data_matrix, distance_function, n_threads)\n",
    "            nearest_neighbour[i] = nearest_index\n",
    "        print(\"Computing scores\")\n",
    "        for distance in dists.dist_range:\n",
    "            clusters = dists.DBSCAN(distance)\n",
    "            for i in range(true_signal.shape[0]):\n",
    "                minimum = i-i%nts_per_clust\n",
    "                maximum = minimum + nts_per_clust\n",
    "                ground_truth = np.ones(nclust*nts_per_clust + nnoise)\n",
    "                ground_truth[minimum:maximum] = 0\n",
    "                prediction = np.ones(nclust*nts_per_clust + nnoise)\n",
    "                for cluster in clusters.values():\n",
    "                    if i in cluster:\n",
    "                        nearest_cluster = cluster\n",
    "                        break\n",
    "                prediction[nearest_cluster] = 0\n",
    "                score = adjusted_rand_score(ground_truth, prediction)\n",
    "                if minimum not in scores:\n",
    "                    scores[minimum] = [score]\n",
    "                else:\n",
    "                    scores[minimum].append(score)\n",
    "        return scores\n",
    "\n",
    "\n",
    "    scores = score(true_signal, dists, data_matrix, nclust, nts_per_clust, nsr)\n",
    "    param_scores[param] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores represent the best clustering result achievable, across all epsilon values, for that given seed signal. More intuitively, this represents the ability to recover the complete set of signals that are sampled/observed from some underlying process, given knowledge of that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.41407416101030137\n",
      "1: 0.43006847911389684\n",
      "2: 0.41256350970337835\n",
      "3: 0.39982869353282574\n",
      "4: 0.39866021852533867\n",
      "5: 0.4224989603062482\n",
      "6: 0.41400331435225274\n",
      "7: 0.4022853386310167\n",
      "8: 0.42283090398663686\n",
      "9: 0.4006370231360688\n",
      "Mean across replicates: 0.411745\n",
      "Std. dev. across replicates: 0.010543\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "total_best = []\n",
    "for param, clusters in param_scores.items():\n",
    "    n+=1\n",
    "    best = []\n",
    "    for cluster_id, scores in clusters.items():\n",
    "        best.append(max(scores))\n",
    "    print(str(param) + \": \" + str(np.mean(best)))\n",
    "    total_best.append(np.mean(best))\n",
    "print(\"Mean across replicates: %f\\nStd. dev. across replicates: %f\" % (np.mean(total_best), np.std(total_best)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the cluster that corresponds to a given tru\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def plot_cluster(true_signal, seed_index, dists):\n",
    "    #Plot some of the clusters\n",
    "    data = []\n",
    "    signal = true_signal[seed_index,:]\n",
    "    observed = tsd._h5t[\"data/timeseries/matrix\"][seed_index,:]\n",
    "    nearest_index = find_nearest_timeseries(signal/sum(signal), \n",
    "                                            data_matrix, distance_function, n_threads=1)\n",
    "    epsilon = None\n",
    "    cluster_id = None\n",
    "    for epsilon in dists.dist_range:\n",
    "        data = [{'name':'signal', 'x': timepoints, 'y': signal/sum(signal)},\n",
    "                {'name':'actual', 'x': timepoints, 'y': observed/sum(observed)}]\n",
    "        cluster_member_indexes = dists.DBSCAN(epsilon, expand_around=nearest_index)\n",
    "        cluster_id = list(cluster_member_indexes.keys())[0]\n",
    "        for ts_id in cluster_member_indexes[cluster_id]:\n",
    "            ts = tsd._h5t[\"data/timeseries/matrix\"][ts_id,:]\n",
    "            data.append({'name':ts_id, 'y': ts/sum(ts), 'x': timepoints})\n",
    "        iplot(data)\n",
    "plot_cluster(true_signal, 2, dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ananke)",
   "language": "python",
   "name": "conda_ananke"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
