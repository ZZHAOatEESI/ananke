{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lagged Time-Series Clustering Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from fastdtw import fastdtw\n",
    "from ananke._database_rework import TimeSeriesData\n",
    "from ananke._ts_simulation import gen_table\n",
    "from ananke._input import fasta_to_ananke\n",
    "from DBloomSCAN import DBloomSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the data using a given distance function. Can do this in memory, or pull the data from disk. Only necessary for really large sets or my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(tsd, distance_function, in_memory=True, n_threads=1):\n",
    "    n_objects = tsd._h5t[\"data/timeseries/matrix\"].shape[0]\n",
    "    \n",
    "    if in_memory:\n",
    "        data_matrix = np.empty(tsd._h5t[\"data/timeseries/matrix\"].shape)\n",
    "        print(\"Loading data matrix into memory\")\n",
    "        tsd._h5t[\"data/timeseries/matrix\"].read_direct(data_matrix)\n",
    "        def retrieve_data(index):\n",
    "            #If the data is too large for RAM, this can be swapped around to read from disk\n",
    "            #data = tsd._h5t[\"data/timeseries/matrix\"][index, :]\n",
    "            #return data/sum(data)\n",
    "            return data_matrix[index,:]/sum(data_matrix[index,:])\n",
    "    else:\n",
    "        def retrieve_data(index):\n",
    "            #If the data is too large for RAM, this can be swapped around to read from disk\n",
    "            data = tsd._h5t[\"data/timeseries/matrix\"][index, :]\n",
    "            return data/sum(data)\n",
    "\n",
    "    print(\"Initializing BloomDistance structure\")\n",
    "    dbl = DBloomSCAN(n_objects, distance_function, retrieve_data, \n",
    "                       dist_min = 0.0001, dist_max=0.015, dist_step=0.0005)\n",
    "    print(\"Pre-computing distances\")\n",
    "    #This should be set to 1 unless you're using DDTW, but I think that crashes anyways.\n",
    "    #Worth a shot somewhere with more RAM.\n",
    "    dbl.compute_distances(n_threads=n_threads)\n",
    "    return dbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsd = TimeSeriesData(\"Mendota.h5\")\n",
    "#tsd.initialize_from_metadata(\"/home/mwhall/Documents/Ananke/Ananke_PeerJ/McMahon_Mendota/sequence_data/METADATA_modified.txt\", name_col=\"#SampleID\", time_col=\"time_points\")\n",
    "#fasta_to_ananke(open(\"/home/mwhall/Documents/Ananke/Ananke_PeerJ/McMahon_Mendota/sequence_data/seq.fasta\"), tsd, push_at=1e9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Origin version: 0.4.0\n",
      "Num. of Time Points: 96\n",
      "Num. of Time Series: 6116298\n",
      "timeseries\n",
      "(35788, 96)\n"
     ]
    }
   ],
   "source": [
    "print(tsd)\n",
    "for group in tsd._h5t[\"data\"]:\n",
    "    print(group)\n",
    "print(tsd._h5t[\"data/timeseries/matrix\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tsd._h5t[\"data/timeseries/matrix\"]\n",
    "def chunks(N, nb):\n",
    "    step = N / nb\n",
    "    return [(round(step*i), round(step*(i+1))) for i in range(nb)]\n",
    "#tsd = TimeSeriesData(\"Mendota.h5\")\n",
    "nrows, ncols = matrix.shape\n",
    "\n",
    "threshold = 20\n",
    "def filter_function(row):\n",
    "    return np.count_nonzero(row) <= threshold\n",
    "\n",
    "cursor = 0\n",
    "#Grab big chunks for efficiency\n",
    "for i, j in chunks(nrows, 10000):\n",
    "    rows = matrix[i:j,:]\n",
    "    \n",
    "    for k in range(i,j):\n",
    "        if not filter_function(rows[k-i,:]):\n",
    "            if k != cursor:\n",
    "                matrix[cursor, :] = rows[k-i,:]\n",
    "            cursor += 1\n",
    "matrix.resize(size=(cursor - 1, ncols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data matrix into memory\n",
      "Initializing BloomDistance structure\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mwhall/anaconda3/envs/ananke/lib/python3.6/site-packages/ipykernel/__main__.py:13: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 71576 samples of the distances, the max distance was 1.951635\n",
      "Pre-computing distances\n",
      "54.01%\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-69372df36861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdistance_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_ddtw_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtsd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#Load once so we don't load it a billion times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f6d5366365a6>\u001b[0m in \u001b[0;36mcompute_distances\u001b[0;34m(tsd, distance_function, in_memory, n_threads)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m#This should be set to 1 unless you're using DDTW, but I think that crashes anyways.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#Worth a shot somewhere with more RAM.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mdbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdbl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ananke/DBloomSCAN.py\u001b[0m in \u001b[0;36mcompute_distances\u001b[0;34m(self, n_threads)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mpercent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_objects\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_objects\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%0.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpercent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-69372df36861>\u001b[0m in \u001b[0;36mcompute_dtw_distance\u001b[0;34m(data1, data2)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_dtw_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdistance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfastdtw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfastdtw/_fastdtw.pyx\u001b[0m in \u001b[0;36mfastdtw._fastdtw.fastdtw (fastdtw/_fastdtw.cpp:2144)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfastdtw/_fastdtw.pyx\u001b[0m in \u001b[0;36mfastdtw._fastdtw.__prep_inputs (fastdtw/_fastdtw.cpp:3799)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "distance_measure = \"dtw\"\n",
    "#time_points = [int(x) for x in tsd._h5t[\"data/timeseries/time\"][:]]\n",
    "#time_delta = np.array(time_points[1:]) - np.array(time_points[0:-1])\n",
    "#data_matrix = np.empty(tsd._h5t[\"data/timeseries/matrix\"].shape)\n",
    "#tsd._h5t[\"data/timeseries/matrix\"].read_direct(data_matrix)\n",
    "\n",
    "def compute_ddtw_distance(data1, data2):\n",
    "    distance, path = DDTW(data1, data2)\n",
    "    distance = distance[-1, -1]\n",
    "    return distance\n",
    "\n",
    "def compute_dtw_distance(data1, data2):\n",
    "    distance, path = fastdtw(data1, data2)\n",
    "    return distance\n",
    "\n",
    "def compute_sts_distance(data1, data2):\n",
    "    data1_delta = np.array(data1[1:]) - np.array(data1[0:-1])\n",
    "    data2_delta = np.array(data2[1:]) - np.array(data2[0:-1])\n",
    "    data1_slope = data1_delta / time_delta\n",
    "    data2_slope = data2_delta / time_delta\n",
    "    distance = data1_slope - data2_slope\n",
    "    distance = np.square(distance)\n",
    "    distance = np.sqrt(sum(distance))\n",
    "    return distance\n",
    "\n",
    "if distance_measure == \"sts\":\n",
    "    distance_function = compute_sts_distance\n",
    "elif distance_measure == \"dtw\":\n",
    "    distance_function = compute_dtw_distance\n",
    "elif distance_measure == \"ddtw\":\n",
    "    distance_function = compute_ddtw_distance\n",
    "\n",
    "dists = compute_distances(tsd, distance_function, in_memory=True, n_threads=1)\n",
    "\n",
    "#Load once so we don't load it a billion times\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores represent the best clustering result achievable, across all epsilon values, for that given seed signal. More intuitively, this represents the ability to recover the complete set of signals that are sampled/observed from some underlying process, given knowledge of that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print out the cluster that corresponds to a given tru\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "def plot_cluster(true_signal, seed_index, dists):\n",
    "    #Plot some of the clusters\n",
    "    data = []\n",
    "    signal = true_signal[seed_index,:]\n",
    "    observed = tsd._h5t[\"data/timeseries/matrix\"][seed_index,:]\n",
    "    nearest_index = find_nearest_timeseries(signal/sum(signal), \n",
    "                                            data_matrix, distance_function, n_threads=1)\n",
    "    epsilon = None\n",
    "    cluster_id = None\n",
    "    for epsilon in dists.dist_range:\n",
    "        data = [{'name':'signal', 'x': timepoints, 'y': signal/sum(signal)},\n",
    "                {'name':'actual', 'x': timepoints, 'y': observed/sum(observed)}]\n",
    "        cluster_member_indexes = dists.DBSCAN(epsilon, expand_around=nearest_index)\n",
    "        cluster_id = list(cluster_member_indexes.keys())[0]\n",
    "        for ts_id in cluster_member_indexes[cluster_id]:\n",
    "            ts = tsd._h5t[\"data/timeseries/matrix\"][ts_id,:]\n",
    "            data.append({'name':ts_id, 'y': ts/sum(ts), 'x': timepoints})\n",
    "        iplot(data)\n",
    "plot_cluster(true_signal, 2, dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranges(N, nb):\n",
    "    step = N / nb\n",
    "    return [(round(step*i), round(step*(i+1))) for i in range(nb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges(6219008, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_ananke)",
   "language": "python",
   "name": "conda_ananke"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
